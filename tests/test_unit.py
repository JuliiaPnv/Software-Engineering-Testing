import pytest
from unittest.mock import patch, MagicMock, mock_open
import subprocess
from parser.utils.pdf_extractor import extract_text_from_pdf
from parser.utils.docx_extractor import extract_text_from_docx
from parser.utils.doc_extractor import extract_text_from_doc
from parser.utils.djvu_extractor import extract_text_from_djvu
from parser.utils.html_extractor import parse_html
from parser.utils.text_analyzer import analyze_text
from parser.Exceptions import LanguageError
from langdetect.lang_detect_exception import LangDetectException

# –¢–µ—Å—Ç—ã –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–∞ –∏–∑ PDF —Ñ–∞–π–ª–æ–≤
def test_extract_text_from_pdf_success():
    """
    –ü—Ä–æ–≤–µ—Ä—è–µ—Ç —É—Å–ø–µ—à–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –∏–∑ PDF —Ñ–∞–π–ª–∞.
    –¢–µ—Å—Ç —Å–∏–º—É–ª–∏—Ä—É–µ—Ç —Ä–∞–±–æ—Ç—É –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ PyMuPDF (fitz) –¥–ª—è —á—Ç–µ–Ω–∏—è PDF.
    """
    mock_document = MagicMock()
    mock_page = MagicMock()
    mock_page.get_text.return_value = "Test text"
    mock_document.load_page.return_value = mock_page
    mock_document.__len__.return_value = 1
    
    with patch('parser.utils.pdf_extractor.fitz.open', return_value=mock_document):
        result = extract_text_from_pdf("test_sample.pdf")
        assert result == "Test text"

def test_extract_text_from_pdf_error():
    """
    –ü—Ä–æ–≤–µ—Ä—è–µ—Ç –æ–±—Ä–∞–±–æ—Ç–∫—É –æ—à–∏–±–∫–∏ –ø—Ä–∏ —á—Ç–µ–Ω–∏–∏ PDF —Ñ–∞–π–ª–∞.
    –¢–µ—Å—Ç –ø—Ä–æ–≤–µ—Ä—è–µ—Ç, —á—Ç–æ —Ñ—É–Ω–∫—Ü–∏—è –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø—É—Å—Ç—É—é —Å—Ç—Ä–æ–∫—É –ø—Ä–∏ –≤–æ–∑–Ω–∏–∫–Ω–æ–≤–µ–Ω–∏–∏ –∏—Å–∫–ª—é—á–µ–Ω–∏—è.
    """
    with patch('parser.utils.pdf_extractor.fitz.open', side_effect=Exception("Test error")):
        result = extract_text_from_pdf("test_sample.pdf")
        assert result == ""

# –¢–µ—Å—Ç—ã –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–∞ –∏–∑ DOCX —Ñ–∞–π–ª–æ–≤
def test_extract_text_from_docx_success():
    """
    –ü—Ä–æ–≤–µ—Ä—è–µ—Ç —É—Å–ø–µ—à–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –∏–∑ DOCX —Ñ–∞–π–ª–∞.
    –¢–µ—Å—Ç —Å–∏–º—É–ª–∏—Ä—É–µ—Ç —Ä–∞–±–æ—Ç—É –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ python-docx –¥–ª—è —á—Ç–µ–Ω–∏—è DOCX.
    """
    mock_document = MagicMock()
    mock_paragraph = MagicMock()
    mock_paragraph.text = "Test text"
    mock_document.paragraphs = [mock_paragraph]
    
    with patch('parser.utils.docx_extractor.Document', return_value=mock_document):
        result = extract_text_from_docx("test_sample.docx")
        assert result == "Test text"

def test_extract_text_from_docx_error():
    """
    –ü—Ä–æ–≤–µ—Ä—è–µ—Ç –æ–±—Ä–∞–±–æ—Ç–∫—É –æ—à–∏–±–∫–∏ –ø—Ä–∏ —á—Ç–µ–Ω–∏–∏ DOCX —Ñ–∞–π–ª–∞.
    –¢–µ—Å—Ç –ø—Ä–æ–≤–µ—Ä—è–µ—Ç, —á—Ç–æ —Ñ—É–Ω–∫—Ü–∏—è –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø—É—Å—Ç—É—é —Å—Ç—Ä–æ–∫—É –ø—Ä–∏ –≤–æ–∑–Ω–∏–∫–Ω–æ–≤–µ–Ω–∏–∏ –∏—Å–∫–ª—é—á–µ–Ω–∏—è.
    """
    with patch('parser.utils.docx_extractor.Document', side_effect=Exception("Test error")):
        result = extract_text_from_docx("test_sample.docx")
        assert result == ""

# –¢–µ—Å—Ç—ã –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–∞ –∏–∑ DOC —Ñ–∞–π–ª–æ–≤
def test_extract_text_from_doc_success():
    """
    –ü—Ä–æ–≤–µ—Ä—è–µ—Ç —É—Å–ø–µ—à–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –∏–∑ DOC —Ñ–∞–π–ª–∞.
    –¢–µ—Å—Ç —Å–∏–º—É–ª–∏—Ä—É–µ—Ç —Ä–∞–±–æ—Ç—É —É—Ç–∏–ª–∏—Ç—ã antiword —á–µ—Ä–µ–∑ subprocess –¥–ª—è —á—Ç–µ–Ω–∏—è DOC.
    """
    mock_process = MagicMock()
    mock_process.stdout = b"Test text"
    
    with patch('parser.utils.doc_extractor.subprocess.run', return_value=mock_process):
        result = extract_text_from_doc("test_sample.doc")
        assert result == "Test text"

def test_extract_text_from_doc_error():
    """
    –ü—Ä–æ–≤–µ—Ä—è–µ—Ç –æ–±—Ä–∞–±–æ—Ç–∫—É –æ—à–∏–±–∫–∏ –ø—Ä–∏ —á—Ç–µ–Ω–∏–∏ DOC —Ñ–∞–π–ª–∞.
    –¢–µ—Å—Ç –ø—Ä–æ–≤–µ—Ä—è–µ—Ç, —á—Ç–æ —Ñ—É–Ω–∫—Ü–∏—è –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø—É—Å—Ç—É—é —Å—Ç—Ä–æ–∫—É –ø—Ä–∏ –≤–æ–∑–Ω–∏–∫–Ω–æ–≤–µ–Ω–∏–∏ –∏—Å–∫–ª—é—á–µ–Ω–∏—è.
    """
    with patch('parser.utils.doc_extractor.subprocess.run', side_effect=Exception("Test error")):
        result = extract_text_from_doc("test_sample.doc")
        assert result == ""

# –¢–µ—Å—Ç—ã –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–∞ –∏–∑ DJVU —Ñ–∞–π–ª–æ–≤
def test_extract_text_from_djvu_success():
    """
    –ü—Ä–æ–≤–µ—Ä—è–µ—Ç —É—Å–ø–µ—à–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –∏–∑ DJVU —Ñ–∞–π–ª–∞.
    –¢–µ—Å—Ç —Å–∏–º—É–ª–∏—Ä—É–µ—Ç —Ä–∞–±–æ—Ç—É —É—Ç–∏–ª–∏—Ç—ã djvutxt —á–µ—Ä–µ–∑ subprocess –¥–ª—è —á—Ç–µ–Ω–∏—è DJVU.
    """
    mock_process = MagicMock()
    mock_process.stdout = b"Test text"
    
    with patch('parser.utils.djvu_extractor.subprocess.run', return_value=mock_process) as mock_run:
        with patch('builtins.open', mock_open(read_data="Test text")):
            result = extract_text_from_djvu("test_sample.djvu")
            assert result == "Test text"
            mock_run.assert_called_once_with(['djvutxt', 'test_sample.djvu', 'output.txt'], 
                                           stdout=subprocess.PIPE, check=True)

def test_extract_text_from_djvu_error():
    """
    –ü—Ä–æ–≤–µ—Ä—è–µ—Ç –æ–±—Ä–∞–±–æ—Ç–∫—É –æ—à–∏–±–∫–∏ –ø—Ä–∏ —á—Ç–µ–Ω–∏–∏ DJVU —Ñ–∞–π–ª–∞.
    –¢–µ—Å—Ç –ø—Ä–æ–≤–µ—Ä—è–µ—Ç, —á—Ç–æ —Ñ—É–Ω–∫—Ü–∏—è –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø—É—Å—Ç—É—é —Å—Ç—Ä–æ–∫—É –ø—Ä–∏ –≤–æ–∑–Ω–∏–∫–Ω–æ–≤–µ–Ω–∏–∏ –∏—Å–∫–ª—é—á–µ–Ω–∏—è.
    """
    with patch('parser.utils.djvu_extractor.subprocess.run', side_effect=Exception("Test error")):
        result = extract_text_from_djvu("test_sample.djvu")
        assert result == ""

# –¢–µ—Å—Ç—ã –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞ HTML —Å—Ç—Ä–∞–Ω–∏—Ü
def test_parse_html_success():
    """
    –ü—Ä–æ–≤–µ—Ä—è–µ—Ç —É—Å–ø–µ—à–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –∏–∑ HTML —Å—Ç—Ä–∞–Ω–∏—Ü—ã.
    –¢–µ—Å—Ç —Å–∏–º—É–ª–∏—Ä—É–µ—Ç —Ä–∞–±–æ—Ç—É –±–∏–±–ª–∏–æ—Ç–µ–∫ requests –∏ BeautifulSoup –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞ HTML.
    """
    mock_response = MagicMock()
    mock_response.status_code = 200
    mock_response.content = b"<html><body><p>Test text</p></body></html>"
    mock_response.apparent_encoding = "utf-8"
    
    with patch('parser.utils.html_extractor.requests.get', return_value=mock_response):
        with patch('parser.utils.html_extractor.BeautifulSoup') as mock_soup:
            mock_paragraph = MagicMock()
            mock_paragraph.text = "Test text"
            mock_soup.return_value.find_all.return_value = [mock_paragraph]
            result = parse_html("http://test.com")
            assert result is None  # –§—É–Ω–∫—Ü–∏—è –Ω–µ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∑–Ω–∞—á–µ–Ω–∏–µ

def test_parse_html_error():
    """
    –ü—Ä–æ–≤–µ—Ä—è–µ—Ç –æ–±—Ä–∞–±–æ—Ç–∫—É –æ—à–∏–±–∫–∏ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ HTML —Å—Ç—Ä–∞–Ω–∏—Ü—ã.
    –¢–µ—Å—Ç –ø—Ä–æ–≤–µ—Ä—è–µ—Ç, —á—Ç–æ —Ñ—É–Ω–∫—Ü–∏—è –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç None –ø—Ä–∏ –≤–æ–∑–Ω–∏–∫–Ω–æ–≤–µ–Ω–∏–∏ –∏—Å–∫–ª—é—á–µ–Ω–∏—è.
    """
    with patch('parser.utils.html_extractor.requests.get', side_effect=Exception("Test error")):
        result = parse_html("http://test.com")
        assert result is None

# –¢–µ—Å—Ç—ã –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —Ç–µ–∫—Å—Ç–∞
def test_analyze_text_success():
    """
    –ü—Ä–æ–≤–µ—Ä—è–µ—Ç —É—Å–ø–µ—à–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Ç–µ–∫—Å—Ç–∞.
    –¢–µ—Å—Ç —Å–∏–º—É–ª–∏—Ä—É–µ—Ç —Ä–∞–±–æ—Ç—É –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ spaCy –¥–ª—è –ª–∏–Ω–≥–≤–∏—Å—Ç–∏—á–µ—Å–∫–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞.
    """
    mock_doc = MagicMock()
    mock_token = MagicMock()
    mock_token.text = "Test"
    mock_token.pos_ = "NOUN"
    mock_token.dep_ = "nsubj"
    mock_token.lemma_ = "test"
    mock_doc.__iter__.return_value = [mock_token]
    
    with patch('parser.utils.text_analyzer.spacy.load') as mock_spacy:
        mock_spacy.return_value.return_value = mock_doc
        result = list(analyze_text("Test text"))
        assert len(result) == 1
        assert result[0]['text'] == "Test"
        assert result[0]['position'] == "NOUN"
        assert result[0]['dependency'] == "nsubj"
        assert result[0]['lemma'] == "test"

def test_analyze_text_unknown_language():
    """
    –ü—Ä–æ–≤–µ—Ä—è–µ—Ç –æ–±—Ä–∞–±–æ—Ç–∫—É —Ç–µ–∫—Å—Ç–∞ –Ω–∞ –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–æ–º —è–∑—ã–∫–µ.
    –¢–µ—Å—Ç –ø—Ä–æ–≤–µ—Ä—è–µ—Ç, —á—Ç–æ —Ñ—É–Ω–∫—Ü–∏—è –≤—ã–±—Ä–∞—Å—ã–≤–∞–µ—Ç LanguageError –ø—Ä–∏ –Ω–µ–≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å —è–∑—ã–∫.
    """
    with patch('parser.utils.text_analyzer.detect', side_effect=LangDetectException(code=0, message="Unknown language")):
        with pytest.raises(LanguageError) as exc_info:
            analyze_text("Test text")
        assert exc_info.value.message == "–ù–µ —É–¥–∞–ª–æ—Å—å –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å —è–∑—ã–∫ —Ç–µ–∫—Å—Ç–∞: Unknown language"

# –ü–∞—Ä–∞–º–µ—Ç—Ä–∏–∑–æ–≤–∞–Ω–Ω—ã–µ —Ç–µ—Å—Ç—ã –¥–ª—è –≥—Ä–∞–º–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞
@pytest.mark.parametrize("text,expected_pos,expected_dep,expected_lemma", [
    ("–∂–∂–∏–ª–æ–π", "ADJ", "amod", "–∂–∏–ª–æ–π"),  # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è –æ–ø–µ—á–∞—Ç–∫–∏
    ("NASA", "PROPN", "nsubj", "NASA"),  # –ü—Ä–æ–≤–µ—Ä–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∞–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä
])
def test_analyze_text_grammar(text, expected_pos, expected_dep, expected_lemma):
    """
    –ü—Ä–æ–≤–µ—Ä—è–µ—Ç –≥—Ä–∞–º–º–∞—Ç–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ç–∏–ø–æ–≤ —Å–ª–æ–≤.
    –¢–µ—Å—Ç –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –ø–∞—Ä–∞–º–µ—Ç—Ä–∏–∑–∞—Ü–∏—é –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ —Ä–∞–∑–Ω—ã—Ö —Å–ª—É—á–∞–µ–≤:
    - –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –æ–ø–µ—á–∞—Ç–æ–∫
    - –û–±—Ä–∞–±–æ—Ç–∫–∞ –∞–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä
    """
    mock_doc = MagicMock()
    mock_token = MagicMock()
    mock_token.text = text
    mock_token.pos_ = expected_pos
    mock_token.dep_ = expected_dep
    mock_token.lemma_ = expected_lemma
    mock_doc.__iter__.return_value = [mock_token]
    
    with patch('parser.utils.text_analyzer.detect', return_value='en'):
        with patch('parser.utils.text_analyzer.spacy.load') as mock_spacy:
            mock_spacy.return_value.return_value = mock_doc
            result = list(analyze_text(text))
            assert len(result) == 1
            assert result[0]['text'] == text
            assert result[0]['position'] == expected_pos
            assert result[0]['dependency'] == expected_dep
            assert result[0]['lemma'] == expected_lemma

# –¢–µ—Å—Ç—ã –≥—Ä–∞–Ω–∏—á–Ω—ã—Ö —Å–ª—É—á–∞–µ–≤ –∏ –æ—Å–æ–±—ã—Ö —Å—Ü–µ–Ω–∞—Ä–∏–µ–≤
def test_analyze_text_empty():
    """
    –ü—Ä–æ–≤–µ—Ä—è–µ—Ç –æ–±—Ä–∞–±–æ—Ç–∫—É –ø—É—Å—Ç–æ–≥–æ —Ç–µ–∫—Å—Ç–∞.
    –ì—Ä–∞–Ω–∏—á–Ω—ã–π —Å–ª—É—á–∞–π: –ø—É—Å—Ç–∞—è —Å—Ç—Ä–æ–∫–∞ –¥–æ–ª–∂–Ω–∞ –≤—ã–∑—ã–≤–∞—Ç—å LanguageError.
    """
    with pytest.raises(LanguageError) as exc_info:
        analyze_text("")
    assert exc_info.value.message == "–¢–µ–∫—Å—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –Ω–µ –º–æ–∂–µ—Ç –±—ã—Ç—å –ø—É—Å—Ç—ã–º."

def test_analyze_text_only_spaces():
    """
    –ü—Ä–æ–≤–µ—Ä—è–µ—Ç –æ–±—Ä–∞–±–æ—Ç–∫—É —Ç–µ–∫—Å—Ç–∞, —Å–æ—Å—Ç–æ—è—â–µ–≥–æ —Ç–æ–ª—å–∫–æ –∏–∑ –ø—Ä–æ–±–µ–ª—å–Ω—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤.
    –ì—Ä–∞–Ω–∏—á–Ω—ã–π —Å–ª—É—á–∞–π: —Å—Ç—Ä–æ–∫–∞ —Å –ø—Ä–æ–±–µ–ª–∞–º–∏, —Ç–∞–±—É–ª—è—Ü–∏—è–º–∏ –∏ –ø–µ—Ä–µ–Ω–æ—Å–∞–º–∏ —Å—Ç—Ä–æ–∫ –¥–æ–ª–∂–Ω–∞ –≤—ã–∑—ã–≤–∞—Ç—å LanguageError.
    """
    with pytest.raises(LanguageError) as exc_info:
        analyze_text("   \t\n  ")
    assert exc_info.value.message == "–¢–µ–∫—Å—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –Ω–µ –º–æ–∂–µ—Ç –±—ã—Ç—å –ø—É—Å—Ç—ã–º."

def test_analyze_text_mixed_language():
    """
    –ü—Ä–æ–≤–µ—Ä—è–µ—Ç –æ–±—Ä–∞–±–æ—Ç–∫—É —Ç–µ–∫—Å—Ç–∞ —Å –¥–≤—É–º—è —è–∑—ã–∫–∞–º–∏.
    –û—Å–æ–±—ã–π —Å–ª—É—á–∞–π: —Ç–µ–∫—Å—Ç —Å–æ–¥–µ—Ä–∂–∏—Ç —Å–ª–æ–≤–∞ –Ω–∞ —Ä–∞–∑–Ω—ã—Ö —è–∑—ã–∫–∞—Ö (–∞–Ω–≥–ª–∏–π—Å–∫–∏–π –∏ —Ä—É—Å—Å–∫–∏–π).
    """
    mock_doc = MagicMock()
    mock_token1 = MagicMock()
    mock_token1.text = "Hello"
    mock_token1.pos_ = "INTJ"
    mock_token1.dep_ = "discourse"
    mock_token1.lemma_ = "hello"
    
    mock_token2 = MagicMock()
    mock_token2.text = "–º–∏—Ä"
    mock_token2.pos_ = "NOUN"
    mock_token2.dep_ = "root"
    mock_token2.lemma_ = "–º–∏—Ä"
    
    mock_doc.__iter__.return_value = [mock_token1, mock_token2]
    
    with patch('parser.utils.text_analyzer.detect', return_value='ru'):
        with patch('parser.utils.text_analyzer.spacy.load') as mock_spacy:
            mock_spacy.return_value.return_value = mock_doc
            result = list(analyze_text("Hello –º–∏—Ä"))
            assert len(result) == 2
            assert result[0]['text'] == "Hello"
            assert result[1]['text'] == "–º–∏—Ä"

def test_analyze_text_special_characters():
    """
    –ü—Ä–æ–≤–µ—Ä—è–µ—Ç –æ–±—Ä–∞–±–æ—Ç–∫—É —Ç–µ–∫—Å—Ç–∞ —Å–æ —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–º–∏ —Å–∏–º–≤–æ–ª–∞–º–∏.
    –û—Å–æ–±—ã–π —Å–ª—É—á–∞–π: —Ç–µ–∫—Å—Ç —Å–æ–¥–µ—Ä–∂–∏—Ç –∑–Ω–∞–∫–∏ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è –∏ –¥—Ä—É–≥–∏–µ —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã.
    """
    mock_doc = MagicMock()
    mock_token = MagicMock()
    mock_token.text = "Hello!"
    mock_token.pos_ = "INTJ"
    mock_token.dep_ = "discourse"
    mock_token.lemma_ = "hello"
    mock_doc.__iter__.return_value = [mock_token]
    
    with patch('parser.utils.text_analyzer.detect', return_value='en'):
        with patch('parser.utils.text_analyzer.spacy.load') as mock_spacy:
            mock_spacy.return_value.return_value = mock_doc
            result = list(analyze_text("Hello!"))
            assert len(result) == 1
            assert result[0]['text'] == "Hello!"

def test_analyze_text_very_long():
    """
    –ü—Ä–æ–≤–µ—Ä—è–µ—Ç –æ–±—Ä–∞–±–æ—Ç–∫—É –æ—á–µ–Ω—å –¥–ª–∏–Ω–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞.
    –ì—Ä–∞–Ω–∏—á–Ω—ã–π —Å–ª—É—á–∞–π: —Ç–µ–∫—Å—Ç —Å–æ–¥–µ—Ä–∂–∏—Ç 5000 —Å–∏–º–≤–æ–ª–æ–≤ (1000 –ø–æ–≤—Ç–æ—Ä—è—é—â–∏—Ö—Å—è —Å–ª–æ–≤).
    """
    long_text = "word " * 1000  # 5000 —Å–∏–º–≤–æ–ª–æ–≤
    mock_doc = MagicMock()
    mock_tokens = []
    for i in range(1000):
        mock_token = MagicMock()
        mock_token.text = "word"
        mock_token.pos_ = "NOUN"
        mock_token.dep_ = "nsubj"
        mock_token.lemma_ = "word"
        mock_tokens.append(mock_token)
    mock_doc.__iter__.return_value = mock_tokens
    
    with patch('parser.utils.text_analyzer.detect', return_value='en'):
        with patch('parser.utils.text_analyzer.spacy.load') as mock_spacy:
            mock_spacy.return_value.return_value = mock_doc
            result = list(analyze_text(long_text))
            assert len(result) == 1000
            assert all(token['text'] == "word" for token in result)

def test_analyze_text_unicode_emojis():
    """
    –ü—Ä–æ–≤–µ—Ä—è–µ—Ç –æ–±—Ä–∞–±–æ—Ç–∫—É —Ç–µ–∫—Å—Ç–∞ —Å —ç–º–æ–¥–∑–∏.
    –û—Å–æ–±—ã–π —Å–ª—É—á–∞–π: —Ç–µ–∫—Å—Ç —Å–æ–¥–µ—Ä–∂–∏—Ç Unicode-—Å–∏–º–≤–æ–ª—ã —ç–º–æ–¥–∑–∏.
    """
    mock_doc = MagicMock()
    mock_token = MagicMock()
    mock_token.text = "Hello üëã"
    mock_token.pos_ = "INTJ"
    mock_token.dep_ = "discourse"
    mock_token.lemma_ = "hello"
    mock_doc.__iter__.return_value = [mock_token]
    
    with patch('parser.utils.text_analyzer.detect', return_value='en'):
        with patch('parser.utils.text_analyzer.spacy.load') as mock_spacy:
            mock_spacy.return_value.return_value = mock_doc
            result = list(analyze_text("Hello üëã"))
            assert len(result) == 1
            assert result[0]['text'] == "Hello üëã"

# –¢–µ—Å—Ç—ã –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —Ç–µ–∫—Å—Ç–∞ —Å –æ—Å–æ–±—ã–º–∏ —Ñ–æ—Ä–º–∞—Ç–∞–º–∏
def test_analyze_text_with_html():
    """
    –ü—Ä–æ–≤–µ—Ä—è–µ—Ç –æ–±—Ä–∞–±–æ—Ç–∫—É —Ç–µ–∫—Å—Ç–∞, —Å–æ–¥–µ—Ä–∂–∞—â–µ–≥–æ HTML-—Ç–µ–≥–∏.
    –û—Å–æ–±—ã–π —Å–ª—É—á–∞–π: —Ç–µ–∫—Å—Ç —Å–æ–¥–µ—Ä–∂–∏—Ç HTML-—Ä–∞–∑–º–µ—Ç–∫—É, –∫–æ—Ç–æ—Ä–∞—è –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å –ø—Ä–æ–∏–≥–Ω–æ—Ä–∏—Ä–æ–≤–∞–Ω–∞.
    """
    mock_doc = MagicMock()
    mock_token = MagicMock()
    mock_token.text = "Hello world"
    mock_token.pos_ = "NOUN"
    mock_token.dep_ = "nsubj"
    mock_token.lemma_ = "hello"
    mock_doc.__iter__.return_value = [mock_token]
    
    with patch('parser.utils.text_analyzer.detect', return_value='en'):
        with patch('parser.utils.text_analyzer.spacy.load') as mock_spacy:
            mock_spacy.return_value.return_value = mock_doc
            result = list(analyze_text("Hello <b>world</b>"))
            assert len(result) == 1
            assert result[0]['text'] == "Hello world"

def test_analyze_text_with_numbers():
    """
    –ü—Ä–æ–≤–µ—Ä—è–µ—Ç –æ–±—Ä–∞–±–æ—Ç–∫—É —Ç–µ–∫—Å—Ç–∞ —Å —á–∏—Å–ª–∞–º–∏ –∏ –µ–¥–∏–Ω–∏—Ü–∞–º–∏ –∏–∑–º–µ—Ä–µ–Ω–∏—è.
    –û—Å–æ–±—ã–π —Å–ª—É—á–∞–π: —Ç–µ–∫—Å—Ç —Å–æ–¥–µ—Ä–∂–∏—Ç —á–∏—Å–ª–æ–≤—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è —Å –µ–¥–∏–Ω–∏—Ü–∞–º–∏ –∏–∑–º–µ—Ä–µ–Ω–∏—è.
    """
    mock_doc = MagicMock()
    mock_tokens = []
    
    # –°–æ–∑–¥–∞–µ–º —Ç–æ–∫–µ–Ω—ã –¥–ª—è "100 –∫–≥"
    mock_token1 = MagicMock()
    mock_token1.text = "100"
    mock_token1.pos_ = "NUM"
    mock_token1.dep_ = "nummod"
    mock_token1.lemma_ = "100"
    
    mock_token2 = MagicMock()
    mock_token2.text = "–∫–≥"
    mock_token2.pos_ = "NOUN"
    mock_token2.dep_ = "nsubj"
    mock_token2.lemma_ = "–∫–∏–ª–æ–≥—Ä–∞–º–º"
    
    mock_tokens.extend([mock_token1, mock_token2])
    mock_doc.__iter__.return_value = mock_tokens
    
    with patch('parser.utils.text_analyzer.detect', return_value='ru'):
        with patch('parser.utils.text_analyzer.spacy.load') as mock_spacy:
            mock_spacy.return_value.return_value = mock_doc
            result = list(analyze_text("100 –∫–≥"))
            assert len(result) == 2
            assert result[0]['text'] == "100"
            assert result[0]['position'] == "NUM"
            assert result[1]['text'] == "–∫–≥"
            assert result[1]['position'] == "NOUN"

def test_analyze_text_with_dates():
    """
    –ü—Ä–æ–≤–µ—Ä—è–µ—Ç –æ–±—Ä–∞–±–æ—Ç–∫—É —Ç–µ–∫—Å—Ç–∞ —Å –¥–∞—Ç–∞–º–∏ –∏ –≤—Ä–µ–º–µ–Ω–µ–º.
    –û—Å–æ–±—ã–π —Å–ª—É—á–∞–π: —Ç–µ–∫—Å—Ç —Å–æ–¥–µ—Ä–∂–∏—Ç –¥–∞—Ç—ã –∏ –≤—Ä–µ–º–µ–Ω–Ω—ã–µ –º–µ—Ç–∫–∏ –≤ —Ä–∞–∑–Ω—ã—Ö —Ñ–æ—Ä–º–∞—Ç–∞—Ö.
    """
    mock_doc = MagicMock()
    mock_tokens = []
    
    # –°–æ–∑–¥–∞–µ–º —Ç–æ–∫–µ–Ω—ã –¥–ª—è "01.01.2024 15:30"
    mock_token1 = MagicMock()
    mock_token1.text = "01.01.2024"
    mock_token1.pos_ = "NUM"
    mock_token1.dep_ = "nummod"
    mock_token1.lemma_ = "01.01.2024"
    
    mock_token2 = MagicMock()
    mock_token2.text = "15:30"
    mock_token2.pos_ = "NUM"
    mock_token2.dep_ = "nummod"
    mock_token2.lemma_ = "15:30"
    
    mock_tokens.extend([mock_token1, mock_token2])
    mock_doc.__iter__.return_value = mock_tokens
    
    with patch('parser.utils.text_analyzer.detect', return_value='ru'):
        with patch('parser.utils.text_analyzer.spacy.load') as mock_spacy:
            mock_spacy.return_value.return_value = mock_doc
            result = list(analyze_text("01.01.2024 15:30"))
            assert len(result) == 2
            assert result[0]['text'] == "01.01.2024"
            assert result[0]['position'] == "NUM"
            assert result[1]['text'] == "15:30"
            assert result[1]['position'] == "NUM"

# –¢–µ—Å—Ç—ã –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –æ—à–∏–±–æ–∫ –ø—Ä–∏ —Ä–∞–±–æ—Ç–µ —Å —Ñ–∞–π–ª–∞–º–∏
def test_extract_text_from_pdf_corrupted():
    """
    –ü—Ä–æ–≤–µ—Ä—è–µ—Ç –æ–±—Ä–∞–±–æ—Ç–∫—É –ø–æ–≤—Ä–µ–∂–¥–µ–Ω–Ω–æ–≥–æ PDF —Ñ–∞–π–ª–∞.
    –ì—Ä–∞–Ω–∏—á–Ω—ã–π —Å–ª—É—á–∞–π: —Ñ–∞–π–ª —Å—É—â–µ—Å—Ç–≤—É–µ—Ç, –Ω–æ –µ–≥–æ —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ –ø–æ–≤—Ä–µ–∂–¥–µ–Ω–æ.
    """
    with patch('parser.utils.pdf_extractor.fitz.open', side_effect=Exception("PDF file is corrupted")):
        result = extract_text_from_pdf("corrupted.pdf")
        assert result == ""

def test_extract_text_from_docx_permission_error():
    """
    –ü—Ä–æ–≤–µ—Ä—è–µ—Ç –æ–±—Ä–∞–±–æ—Ç–∫—É –æ—à–∏–±–∫–∏ –¥–æ—Å—Ç—É–ø–∞ –∫ DOCX —Ñ–∞–π–ª—É.
    –ì—Ä–∞–Ω–∏—á–Ω—ã–π —Å–ª—É—á–∞–π: —Ñ–∞–π–ª —Å—É—â–µ—Å—Ç–≤—É–µ—Ç, –Ω–æ –Ω–µ—Ç –ø—Ä–∞–≤ –Ω–∞ –µ–≥–æ —á—Ç–µ–Ω–∏–µ.
    """
    with patch('parser.utils.docx_extractor.Document', side_effect=PermissionError("Access denied")):
        result = extract_text_from_docx("restricted.docx")
        assert result == ""

def test_extract_text_from_doc_encoding_error():
    """
    –ü—Ä–æ–≤–µ—Ä—è–µ—Ç –æ–±—Ä–∞–±–æ—Ç–∫—É –æ—à–∏–±–∫–∏ –∫–æ–¥–∏—Ä–æ–≤–∫–∏ –ø—Ä–∏ —á—Ç–µ–Ω–∏–∏ DOC —Ñ–∞–π–ª–∞.
    –ì—Ä–∞–Ω–∏—á–Ω—ã–π —Å–ª—É—á–∞–π: —Ñ–∞–π–ª –∏–º–µ–µ—Ç –Ω–µ–≤–µ—Ä–Ω—É—é –∫–æ–¥–∏—Ä–æ–≤–∫—É.
    """
    with patch('parser.utils.doc_extractor.subprocess.run', side_effect=UnicodeDecodeError("utf-8", b"", 0, 1, "invalid continuation byte")):
        result = extract_text_from_doc("invalid_encoding.doc")
        assert result == "" 